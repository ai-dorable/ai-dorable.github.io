---
layout: post
title: SimCSE ◀️ Simple Contrastive learning of Sentence Embedding
subtitle: 이제 sentence embedding은 이걸로?
style: border
color: danger
tags: [sentence embedding, contrastive learning, review]
description: 이번 포스팅은 EMNLP2021에 발표된 contrastive learning을 적용한 sentence embedding 방법론을 소개하려 한다.
---



# 1. Introduction

SimCSE는 기존 vision 도메인에서 제시된 Contrastive learning 방법론인 SimCLR을 NLP 도메인으로 옮겨온 연구 결과라고 할 수 있다. RoBERTa와 SpanBERT의 저자로 알려진 Danqi Chen 교수의 SimCSE 논문에서는 아래와 같은 결과를 주장하고 있다. 

- 논문에서는 BERT나 RoBERTa 같은 pre-trained 언어모델에서 contrastive objective로 학습하는 것이 효과적일 수 있음을 보여주었다. 
- unsupervised와 supervised하게 sentence embedding이 가능한 simCSE를 제안하였다. 
- unsupervised한 방법으로는 dropout만으로 positive pair를 만들어 minimal한 data augmentation이 가능하도록 유도하며 representation collapse를 막는다는 것을 발견함
- supervised한 방법으로는 NLI 데이터 셋의 sentence pair label을 활용해 contrastive learning을 진행한다. 특히, hart negative pair로 contradiction label을 사용함으로써 상당한 개선을 확인함. 
- semantical하게 생성된 positive pair와 전체 임베딩 공간의 *uniformity*와 *alignment*를 통해 학습된 임베딩의 품질을 측정한다. unsupervised한 학습에서의 dropout을 통한 positive pair 생성 방식이 이러한  representation의 표현성을 향상시킨다고함. 
- 그리고 PLM의 word embedding이 고질적인 anisotropic한 성질을 갖는다는 것과 관련하여 constrastive learning이 문장 embedding 공간의 분포를 flatten 시켜 uniformity를 증가시킨다는 것을 증명하고 있다. 



# 2. Backgroud: Contrastive learning

Contrastive learning의 목표는 의미론적으로 이웃은 가깝게, 그렇지 않은 것은 멀리 위치하도록 representaion 하는 것을 목표로 학습되는 방식이다. $\mathcal{D} = \{(x_I, x_i^+)\}_{i=1}^{m}$이라는 pair example을 가정하고 여기서 $x_i$와 $x_i^+$를 의미적으로 연관있는 pair이다. 여기서 $\bf{h}_i$, $\bf{h}_i^+$는 $x_i$와 $x_i^+$의 representation 벡터를 의미하고 이때 ( $x_i, x_i^+$)에 대한 mini-batch N pair의 loss term은 다음과 같다.


$$
l_i = -log\frac{e^{sim(h_i, h_i^+)/\tau}}{\sum^N_{j=1}e^{sim(h_i, h_j^+)/\tau}}
$$




